---
title: "Debug SVM"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include = FALSE} 
knitr::opts_chunk$set(echo = TRUE)
```


```{r, libraries, message=FALSE}

rm(list = ls())

library(readr)
library(tibble)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(Seurat)
library(caret)
library(data.table)
library(matrixStats)
library(tictoc)
library(MazamaCoreUtils)        # Used for logging messages to a file

```


```{r, initialise, message=FALSE}

PCA_Threshold   <- 0.90         # The percentage variance explained by the PCA
                                # A high percentage requires more PC dimensions to be considered

features_limit  <- 1000         # The number of features that will be considered duringthe PCA
                                # A high number use sthe richness of the data set optimally, but makes processing slo

VERBOSE         <- TRUE         # If TRUE generates analysis data
SEURAT_VERBOSE  <- FALSE
VARIABLE_GENES  <- TRUE
LOGGER          <- FALSE

TRAIN_SVML      <- FALSE
TRAIN_GBM       <- TRUE
TRAIN_GLMNET    <- FALSE

# Summary results will be kept here 
report_out <- data.frame(Method     = character(),
                         ModelData  = character(),
                         TestData   = character(),
                         Accuracy   = numeric(),
                         Confidence = numeric())

# Information of all the runs will be gathered in the class_summaries list  
class_summaries <- list()

```


# Read in the data

"Baron_Mouse",  1,886 cells, 14,861 genes
"Baron_Human",  8,569 cells, 17,499 genes
"Muraro",       2,122 cells, 18,915 genes  
"SegerStolpe",  2,133 cells, 22,757 genes
"Xin",          1,419 cells, 33,889 genes
"Zhou"
"Bo"
"Petropoulos1"
"Petropoulos2"

```{r, Read}

dataset_name     <- "Xin"

preproc_method   <- "pp_normal"
#preproc_method <- "pp_normal"
#preproc_method <- "pp_no_pca"

proj_path   <- file.path(".")
data_path   <- file.path(proj_path, "DataSets")
log_path    <- file.path(proj_path, "Logs")
rdata_path  <- file.path(proj_path, "rData")
config_path <- file.path(proj_path, "config")

logger.setup (infoLog = file.path(log_path, paste0(dataset_name, "_model_compare.log")))

source("Code/Train/read_dataset.R")

# Remove small classes
min_class_size      <- 10
removed_classes     <- !(table(labels$ident) > min_class_size)
cells_to_keep       <- !(is.element(labels[,1], names(removed_classes)[removed_classes]))
ori                 <- dim(data)[1] 
data                <- data[cells_to_keep,]
labels              <- labels[cells_to_keep,]
labels$ident        <- factor(labels$ident)
rm(removed_classes, cells_to_keep)

# Show composition
cat(sprintf("Information on dataset after removing small classes"))
cat(sprintf("Removed %d cells from small classes\n\n", ori - dim(data)[1] )) 

cat(sprintf("There are %d cells and %d features.\n", dim(data)[1], dim(data)[2]))
for (i in 1:length(table(labels$ident))) {
  cat(sprintf("%-25s %d\n", names(table(labels$ident)[i]), table(labels$ident)[i]))
}
cat(sprintf("\n"))

# Remove genes that are 0 for all cells
genes_to_keep <- (colSums(data) != 0)
cat(sprintf("\n"))
ori           <- dim(data)[2]
data          <- data[, genes_to_keep]

# Show composition
cat(sprintf("Information on processed dataset %s\n", dataset_name))
cat(sprintf("Removed %d all-zero genes\n\n",ori - dim(data)[2])) 
cat(sprintf("There are %d cells and %d features.\n\n", dim(data)[1], dim(data)[2]))
for (i in 1:length(table(labels$ident))) {
  cat(sprintf("%-25s %d\n", names(table(labels$ident)[i]), table(labels$ident)[i]))
}

```


# Find the Variable Genes 

```{r, variable_features, fig.height=5, message = FALSE, echo=FALSE}

if (VARIABLE_GENES) {
  seurat_object <- CreateSeuratObject(counts    = t(data),
                                      meta.data = as.data.frame(labels),
                                      min.cells = 5)
  
  seurat_object <- NormalizeData(seurat_object, 
                                 normalization.method = "LogNormalize", 
                                 scale.factor = 1000000)
  
  seurat_object <- FindVariableFeatures(object           = seurat_object, 
                                        selection.method = 'vst',
                                        verbose          = TRUE,
                                        nfeatures        = dim(data)[2])
  
  # Identify the most highly variable genes
  top_genes      <- VariableFeatures(seurat_object)
  
  # Show composition
  cat(sprintf("Information on dataset after variable gene determination of dataset %s\n", dataset_name))
  cat(sprintf("Removed %d genes that apparently are not very variable \n\n", dim(data)[2] - length(top_genes) )) 
  
  data = data[ , top_genes]
  cat(sprintf("\nThere are %d cells and %d features.\n", dim(data)[1], dim(data)[2]))
  for (i in 1:length(table(labels$ident))) {
    cat(sprintf("%-25s %d\n", names(table(labels$ident)[i]), table(labels$ident)[i]))
  }
  cat(sprintf("\n"))
  
  if (SEURAT_VERBOSE) {
    
    # plot variable features with and without labels
    p1 <- VariableFeaturePlot(seurat_object)
    p2 <- LabelPoints(plot   = p1, 
                      points = top_genes[1:15], 
                      repel  = TRUE)
    grid.arrange(p1, p2, nrow = 1)
    rm(p1, p2)
    
    cluster_resolution = 0.5
    
    source("Code/Train/seurat_analysis.R")
    FeatureScatter(seurat_object, feature1 = "nCount_RNA", feature2 = "nFeature_RNA") + NoLegend()
    VlnPlot(seurat_object, features = c("nFeature_RNA", "nCount_RNA"), ncol = 2)
    DimPlot(seurat_object, reduction = 'pca')
    DimPlot(seurat_object, reduction = 'tsne')
    DimPlot(seurat_object, reduction = 'umap', label = TRUE) + NoLegend()
    VizDimLoadings(seurat_object, dims = 1:4, reduction = 'pca')
    DimHeatmap(seurat_object, dims = 1:15, cells = 500, balanced = TRUE)
    ElbowPlot(seurat_object)
  }
}

```


# Caret processing
Here starts the actual training and prediction processing 


```{r}

save_data   <- data
save_labels <- labels

```



### svm Linear test


```{r, svm_Linear, fig.width = 5}
  
if (TRAIN_SVML) {
  
  set.seed(1234)
  
  data   <- save_data
  labels <- save_labels
  method <- "svmLinear"

  # Limit the data set to the number of genes required 
  ldata  <- as.data.frame(data[ , top_genes[1:features_limit]])
  
  # Add labels to ldata
  ldata              <- cbind.data.frame(labels, ldata)
  ldata              <- ldata %>% select(-one_of("cell_id"))

  # Create partions
  trainRowNumbers    <- createDataPartition(ldata$ident, p = 0.8, list = FALSE)
  train_data         <- ldata[trainRowNumbers,]
  test_data          <- ldata[-trainRowNumbers,]

  
  if (preproc_method == "pp_normal") {
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center', 'pca'),
                                thresh  = PCA_Threshold,
                                verbose = TRUE)
    cat(sprintf("\n\nAfter PCA there are %d Principle Components\n\n", dim(preproc_model$rotation)[2]))
  } else if (preproc_method == "pp_no_pca") {
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center'),
                                verbose = TRUE)
  } else if (preproc_method == "pp_log_pca") {
    data <- log(1 + data)
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center', 'pca'),
                                thresh  = PCA_Threshold,
                                verbose = TRUE)
  } else { 
    stop("No valid preprocess mode")
  }
  
  pp_train_data <- predict(preproc_model, train_data)
 
  # Train
  model_svmLinear <- train(ident ~ ., 
                           data      = pp_train_data, 
                           method    = method,
                           trControl = trainControl("cv", 
                                                    number = 5, 
                                                    classProbs = TRUE,
                                                    verboseIter = TRUE))

  
  ###########################################################################
  # Predict and Evaluate
  ###########################################################################
  
  # A few interface variables need to be set in order to be able to call evaluate.R
  # data need to hold the test data set
  # labels need to hold the corresponsing labels 
  
  
  model_name  <- dataset_name
  model       <- model_svmLinear
  labels      <- test_data[,1:2]
  data        <- test_data
  
  #-------------------------------------------------
  source("Code/Predict/predict_model_with_labels.R")
  #-------------------------------------------------

  cat(sprintf("\nPercentage correct after setting a threshold of %2.1f moves from %2.1f%% to %2.1f%% (%2.f%% unassigned)\n", 
              min_prob_value,
              correct_predicted_percentage,
              correct_assigned_percentage,
              unassigned_percentage))
  
  cm <- confusionMatrix(test_data[,1],
                        predicted_classes,
                        mode = "everything",
                        dnn = c("Reference", "Predicted"))
  print(cm)
  
  svml_accuracy <- cm[["overall"]][["Accuracy"]]
  
  F1_1          <- cm[["byClass"]][,c("Precision","Recall")]
  F1_2          <- F1_1[complete.cases(F1_1),]
  F1_3          <- 2 * (F1_2[ , "Precision"] * F1_2[ , "Recall"]) / (F1_2[ ,"Precision"] + F1_2[ ,"Recall"])
  svml_F1       <- median(F1_3, na.rm = TRUE)
  
  knitr::kable(class_summary, digits = 3, caption = "Class Summary overview")

}

```



### GBM


```{r, gbm, , fig.width = 5}
model_gbm    <- NULL
gbm_accuracy <- 0
  
if (TRAIN_GBM) {
  
  set.seed(1234)
  
  data   <- save_data
  labels <- save_labels
  method <- "gbm"
  
  # Limit the data set to the number of genes specified in 'features_limit' (1000 has been found to be a good choice)
  ldata = as.data.frame(data[ , top_genes[1:features_limit]])
  
  ldata              <- cbind.data.frame(labels, ldata)
  ldata              <- ldata %>% select(-one_of("cell_id"))
  trainRowNumbers    <- createDataPartition(ldata$ident, p = 0.8, list = FALSE)
  
  train_data <- ldata[trainRowNumbers,]
  test_data  <- ldata[-trainRowNumbers,]
  
    if (preproc_method == "pp_normal") {
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center', 'pca'),
                                thresh  = PCA_Threshold,
                                verbose = TRUE)
    cat(sprintf("\n\nAfter PCA there are %d Principle Components\n\n", dim(preproc_model$rotation)[2]))
  } else if (preproc_method == "pp_no_pca") {
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center'),
                                verbose = TRUE)
  } else if (preproc_method == "pp_log_pca") {
    data <- log(1 + data)
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center', 'pca'),
                                thresh  = PCA_Threshold,
                                verbose = TRUE)
  } else { 
    stop("No valid preprocess mode")
  }
  
  pp_train_data <- predict(preproc_model, train_data)

  model_gbm <- train(ident ~ ., 
                     data      = pp_train_data, 
                     method    = method,
                     trControl = trainControl("cv", 
                                              number = 5, 
                                              classProbs = TRUE,
                                              verboseIter = TRUE)) 
  

  ###########################################################################
  # Predict and Evaluate
  ###########################################################################
  
  # A few interface variables need to be set in order to be able to call evaluate.R

  model_name  <- dataset_name
  model       <- model_gbm
  labels      <- test_data[,1:2]
  data        <- test_data
  
  #--------------------------------------------------------------------------
  source("Code/Predict/predict_model_with_labels.R")
  #--------------------------------------------------------------------------
  
  cat(sprintf("Percentage correct after setting a threshold of %2.1f moves from %2.1f%% to %2.1f%% (%2.f%% unassigned)\n", 
              min_prob_value,
              correct_predicted_percentage,
              correct_assigned_percentage,
              unassigned_percentage))
  
  cm <- confusionMatrix(test_data[,1],
                        predicted_classes,
                        mode = "everything",
                        dnn = c("Reference", "Predicted"))
  print(cm)
  gbm_accuracy <- cm[["overall"]][["Accuracy"]]
  
  F1_1         <- cm[["byClass"]][,c("Precision","Recall")]
  F1_2         <- F1_1[complete.cases(F1_1),]
  F1_3         <- 2 * (F1_2[,"Precision"] * F1_2[,"Recall"]) / (F1_2[,"Precision"] + F1_2[,"Recall"])
  gbm_F1       <- median(F1_3, na.rm = TRUE)
  knitr::kable(class_summary, digits = 3, caption = "Class Summary overview")
 
}
```


### glmnet

```{r, glmnet , fig.width = 5}

if (TRAIN_GLMNET) {
  
  set.seed(1234)
  
  data   <- save_data
  labels <- save_labels
  method <- "glmnet"

  # Limit the data set to the number of genes required 
  ldata = as.data.frame(data[ , top_genes[1:features_limit]])
  
  # Add labels to ldata
  ldata              <- cbind.data.frame(labels, ldata)
  ldata              <- ldata %>% select(-one_of("cell_id"))

  # Limit the data set to the number of genes specified in 'features_limit' (1000 has been found to be a good choice)
  ldata = as.data.frame(data[ , top_genes[1:features_limit]])
  ldata              <- cbind.data.frame(labels, ldata)
  ldata              <- ldata %>% select(-one_of("cell_id"))
  
  # Create partions
  trainRowNumbers    <- createDataPartition(ldata$ident, p = 0.8, list = FALSE)
  train_data         <- ldata[trainRowNumbers,]
  test_data          <- ldata[-trainRowNumbers,]

    if (preproc_method == "pp_normal") {
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center', 'pca'),
                                thresh  = PCA_Threshold,
                                verbose = TRUE)
    cat(sprintf("\n\nAfter PCA there are %d Principle Components\n\n", dim(preproc_model$rotation)[2]))
  } else if (preproc_method == "pp_no_pca") {
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center'),
                                verbose = TRUE)
  } else if (preproc_method == "pp_log_pca") {
    data <- log(1 + data)
    preproc_model <- preProcess(train_data, 
                                method  = c('scale', 'center', 'pca'),
                                thresh  = PCA_Threshold,
                                verbose = TRUE)
  } else { 
    stop("No valid preprocess mode")
  }
  
  pp_train_data <- predict(preproc_model, train_data)
  
  # Train
  model_glmnet <- train(ident ~ ., 
                        data      = pp_train_data, 
                        method    = method,
                        tuneGrid  = expand.grid(
                          alpha     = 0:1,
                          lambda    = 0.10 / 10
                        ),
                        trControl = trainControl("cv", 
                                                 number = 5, 
                                                 classProbs = TRUE,
                                                 verboseIter = TRUE))

  
  ###########################################################################
  # Predict and Evaluate
  ###########################################################################
  
  # A  few interface variables need to be set in order to be able to call evaluate.R
  
  model_name  <- dataset_name
  model       <- model_glmnet
  labels      <- test_data[,1:2]
  data        <- test_data
  
  #--------------------------------------------------------------------------
  source("Code/Predict/predict_model_with_labels.r")
  #--------------------------------------------------------------------------
  
  cat(sprintf("\nPercentage correct after setting a threshold of %2.1f moves from %2.1f%% to %2.1f%% (%2.f%% unassigned)\n", 
              min_prob_value,
              correct_predicted_percentage,
              correct_assigned_percentage,
              unassigned_percentage))
  
  cm <- confusionMatrix(test_data[,1],
                        predicted_classes,
                        mode = "everything",
                        dnn = c("Reference", "Predicted"))
  print(cm)
  
  glm_accuracy <- cm[["overall"]][["Accuracy"]]
  
  F1_1         <- cm[["byClass"]][,c("Precision","Recall")]
  F1_2         <- F1_1[complete.cases(F1_1),]
  F1_3         <- 2 * (F1_2[,"Precision"] * F1_2[,"Recall"]) / (F1_2[,"Precision"] + F1_2[,"Recall"])
  glm_F1       <- median(F1_3, na.rm = TRUE)
  
  knitr::kable(class_summary, digits = 3, caption = "Class Summary overview")

}
  
```







