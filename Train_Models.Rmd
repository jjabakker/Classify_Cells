---
title: "Train Models"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include = FALSE} 
knitr::opts_chunk$set(echo = TRUE)
```


For a full suite you need to run this 24 times: 3 times for the preproc modes and 8 times for the datasets 
You create 24 rData files containing each 4 models


```{r}
rm(list = ls())
set.seed(1234)
```

# Specify key use parameters

Model options:
Baron_Mouse  # 1,886 cells, 14,861 genes
Baron_Human  # 8,569 cells, 17,499 genes
Muraro       # 2,122 cells, 18,915 genes
SegerStolpe  # 2,133 cells, 22,757 genes
Xin          # 1,449 cells, 33,889 genes
Zhou         # 2,554 cells, 19,046 genes
Bo           # 516   cells, 56,924 genes
Petropoulos1 # 1,529 cells, 26,178 genes 


```{r}

# Specify  which model to train
dataset_name   <- "Baron_Human"

# Select preprocessing

preproc_method <- "pca"
preproc_method <- "nopca"
preproc_method <- "logpca"

preproc_method <- "pca"

# Specify which models should be run
# Note that it possible that a model is not available in the modelset, in which case the instruction is simply ignored

TRAIN_SVML     <- FALSE
TRAIN_SVMR     <- FALSE
TRAIN_RF       <- FALSE
TRAIN_GBM      <- TRUE
TRAIN_GLMNET   <- FALSE

```


# Initialise 

```{r, libraries, message=FALSE}

library(readr)
library(tibble)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(Seurat)
library(caret)
library(data.table)
library(matrixStats)
library(tictoc)                 # Used for timing of code
library(MazamaCoreUtils)        # Used for logging messages to a file

source("Code/Train/read_dataset.R")
source("Code/Train/process_config.R")
source('Code/Train/cluster_utility.R')

```


```{r, General initialisation}

PCA_Threshold  <- 0.90          # The percentage variance explained by the PCA
                                # A high percentage requires more PC dimensions to be considered

features_limit <- 1000          # The number of features that will be considered during the PCA
                                # A high number uses the richness of the data set optimally, but makes processing slow

# Note:
#   TRAIN_GLMNET seems to bring the computer to its knees, especially wgen wroting the results. 
#   Better to run Direct_Compare_Models where nothing is written 

SAVE           <- FALSE

```


```{r, path}

# Set directory paths  
proj_path   <- file.path(".")
data_path   <- file.path(proj_path, "DataSets")
log_path    <- file.path(proj_path, "Logs")
rdata_path  <- file.path(proj_path, "rData")
config_path <- file.path(proj_path, "config")

```


```{r, timer}

# Initialise logging and timer
logger.setup (infoLog = file.path(log_path, paste0(dataset_name, "_", 
                                                   preproc_method, "_model.log")))
tic.clear()
tic.clearlog()

```


```{r, preprocess options}

if (preproc_method == "pca")  {
  log_preprocess   <- FALSE
  caret_preprocess_method <- c('scale', 'center', 'pca')
} else if (preproc_method == "nopca")  {
  log_preprocess   <- FALSE
  caret_preprocess_method <- c('scale', 'center')
} else if (preproc_method == "logpca")  {
  log_preprocess   <- TRUE
  caret_preprocess_method <- c('scale', 'center', 'pca')
}

```


# Read in the data and process

## Read file

```{r, read dataset}

ret          <- read_dataset(data_path, dataset_name)
data         <- ret$data
labels       <- ret$labels

saved_data   <- data
saved_labels <- labels 

if (log_preprocess) {
  data <- log(1 + data)
}

```


## Read the configuration file and process data 

A number of adjustments need to be made to the data and labels (either from a single or double source)
In a configuration file a set parameters are read in:

- min_class_size      Ignore cells if they belong to very small classes.	Special value  is -1 if you do not want to pose a minimum
- cluster_resolution	A higher number will generate more clusters	(typicaly a nummber between 0.1 and 2)
- minCount	          Discard cells if they have less than minCount transcripts	
- maxCount	          Discard cells if they have more than maxCount transcripts	
- minFeature	        Discard cells if they have less than minCount genes (features)	
- maxFeature	        Discard cells if they have more than maxfeature genes (features)	


```{r}

ret                 <- read_config(config_path  = config_path,
                                   dataset_name = dataset_name)
config_data         <- ret$config_data
  
min_class_size      <- config_data["min_class_size", "Value"]
cluster_resolution  <- config_data["cluster_resolution", "Value"] 
minCount            <- config_data["minCount", "Value"] 
maxCount            <- config_data["maxCount", "Value"]
minFeature          <- config_data["minFeature", "Value"] 
maxFeature          <- config_data["maxFeature", "Value"] 

########################################
# Overrule settings from config file
########################################

min_class_size      <- 10 
minCount            <- 0 
maxCount            <- -1
minFeature          <- 0 
maxFeature          <- -1 
  
ret                 <- process_config(min_class_size, minCount, maxCount, minFeature, maxFeature, data, labels)
data                <- ret$data
labels              <- ret$labels

```


# Seurat Find the Variable Genes 

```{r, variable_features, fig.height=5, message = FALSE, echo=FALSE}

seurat_object <- CreateSeuratObject(counts    = t(data),
                                    meta.data = as.data.frame(labels),
                                    min.cells = 5)

seurat_object <- NormalizeData(seurat_object, 
                               normalization.method = "LogNormalize", 
                               scale.factor = 1000000)

seurat_object <- FindVariableFeatures(object           = seurat_object, 
                                      selection.method = 'vst',
                                      nfeatures        = dim(data)[2])

# Identify the most highly variable genes
top_genes     <- VariableFeatures(seurat_object)

# Show composition
cat(sprintf("\n\nInformation on dataset after variable gene determination of dataset %s\n", dataset_name))
cat(sprintf("Removed %d genes that apparently are not very variable \n\n", dim(data)[2] - length(top_genes) )) 

data = data[, top_genes]
cat(sprintf("\nThere are %d cells and %d features.\n", dim(data)[1], dim(data)[2]))
for (i in 1:length(table(labels$ident))) {
  cat(sprintf("%-25s %d\n", names(table(labels$ident)[i]), table(labels$ident)[i]))
}
cat(sprintf("\n"))
  
# Plot variable features with and without labels
p1 <- VariableFeaturePlot(seurat_object)
p2 <- LabelPoints(plot   = p1, 
                  points = top_genes[1:15], 
                  repel  = TRUE)
grid.arrange(p1, p2, nrow = 1)
rm(p1, p2)

```


# Seurat analysis (optional)

```{r, seurat_init}

seurat_object  <- ScaleData(seurat_object, features = rownames(seurat_object))
seurat_object  <- RunPCA(seurat_object, features = VariableFeatures(seurat_object))
seurat_object  <- RunTSNE(seurat_object)
seurat_object  <- RunUMAP(seurat_object, dims = 1:15)
seurat_object  <- FindNeighbors(seurat_object, dims = 1:15)     

```

First run FindNeighbors to compute a snn graph
If there is no pca information, this fails 
Then find the clusters with FindClusters
The resolution parameter determines how many clusters you get
A smaller number is less clusters


```{r, seurat_cluster, fig.height=5,}

sprintf("You'd expect a maximum of %d clusters\n\n", length(table(labels$ident)))
sprintf("Check cluster resolution. Current value is %2.1f\n", cluster_resolution)

ret                  <- find_best_resolution(seurat_object) 
best_resolution      <- ret$best_resolution
seurat_object        <- FindClusters(seurat_object, resolution = best_resolution)
head(x = Idents(seurat_object), 8)

xy                   <- as.data.frame(seurat_object@reductions[["umap"]]@cell.embeddings)
class                <- as.data.frame(seurat_object@meta.data[["ident"]])
colnames(class)[1]   <- "Class"
cluster              <- as.data.frame(seurat_object@meta.data[["seurat_clusters"]])
colnames(cluster)[1] <- "Cluster"
plot_data            <- cbind(xy, class, cluster)

p1 <- ggplot(plot_data, aes(x = UMAP_1, y = UMAP_2, col = Cluster)) +
  geom_point(size = 0.3) +
  theme_light()

nr_of_clusters <- length(levels(seurat_object@active.ident[1]))
for (i in 0:(nr_of_clusters - 1)) {
  cluster  <- labels[WhichCells(seurat_object, idents = i), "cell_id"]
  length(cluster) 
  cluster_coord   <- xy[cluster,]
  cluster_center <- colMeans(cluster_coord)
  p1 <- p1 + annotate("text", x = cluster_center[1], y = cluster_center[2], label= i)
}

p2 <- ggplot(plot_data, aes(x = UMAP_1, y = UMAP_2, col = Class)) +
  geom_point(size = 0.3) +
  theme_light()
grid.arrange(p1, p2, nrow = 1)

ret <- calc_coherence(seurat_object)
print(ret$cluster_table)
```



# Caret processing

Here starts the actual training and prediction processing 


```{r}

logger.info("Info: Started Caret processing data of dataset %s", dataset_name)
tic(msg = "", quiet = TRUE)

```


## Create the training and test set

```{r, create_sets}

# Limit the data set to the number of genes specified in 'features_limit' (1000 has been found to be a good choice)
ldata            <- as.data.frame(data[top_genes[1:features_limit]])
ldata            <- cbind.data.frame(labels, ldata)
ldata            <- ldata %>% select(-one_of("cell_id"))
trainRowNumbers  <- createDataPartition(ldata$ident, p = 0.8, list = FALSE)

train_data       <- ldata[trainRowNumbers,]
test_data        <- ldata[-trainRowNumbers,]

```


## Save the data and labels 

```{r, save_data}

# The data, labels and trainRowNumbers are saved. The variable trainRowNumbers is needed to create a test set

if (SAVE) {
  data_information = list(
    data            = saved_data,
    labels          = saved_labels,
    trainRowNumbers = trainRowNumbers,
    dataset_name    = dataset_name
    )
  
  save(data_information,
       file = file.path(rdata_path, paste0(dataset_name, "_data_and_labels.rData")))
}

```


## Eliminate the genes for which there is no variance

```{r, eliminate_nzv}

# This test is still necessary, because you splitting into train and test sets may have caused
# zero genes in the train set

ngenes     <- dim(train_data)[2]
nzv        <- nearZeroVar(train_data, saveMetrics = TRUE)
train_data <- train_data[, which(!nzv$zeroVar)]
test_data  <- test_data[, which(!nzv$zeroVar)]

if (dim(train_data)[2] != ngenes) {
  cat(sprintf("\n\nFound invariable genes in the train dataset. Removed %d genes.\n", dim(train_data)[2] - ngenes))
} else {
  cat(sprintf("\n\nFound no invariable genes in the train dataset"))
}
```


## Preprocess

```{r, prepocess}

preproc_model <- preProcess(train_data, 
                            method  = caret_preprocess_method,
                            thresh  = PCA_Threshold,
                            verbose = TRUE)

pp_train_data <- predict(preproc_model, train_data)
pp_test_data  <- predict(preproc_model, test_data)

```


```{r}

toc(log=TRUE, quiet = TRUE)
logger.info("Info: Finished Caret processing data of dataset %s%s", dataset_name, tic.log()[[1]])
tic.clearlog()

```


## Do the training

### Support Vector Machine Linear


```{r, model_svmLinear}

model_svmLinear <- NULL
svml_accuracy   <- 0
svml_medianF1   <- 0
  
if (TRAIN_SVML) {
  logger.info("Info: Started training svmLinear with dataset %s", dataset_name)
  tic(msg = "", quiet = TRUE)
  
  model_svmLinear <- train(ident ~ ., 
                           data      = pp_train_data, 
                           method    = "svmLinear",
                           trControl = trainControl("cv", 
                                                    number = 5, 
                                                    classProbs = TRUE,
                                                    verboseIter = TRUE))
  
  print(model_svmLinear)
  print(model_svmLinear$finalModel)
  plot(varImp(model_svmLinear), main="Variable Importance")
  
  predicted_classes <- predict(model_svmLinear, pp_test_data[,-1])
  
  cm <- confusionMatrix(pp_test_data[,1],
                        predicted_classes,
                        mode = "everything",
                        dnn=c("Reference", "Predicted"))
  cm
  svml_accuracy <- cm[["overall"]][["Accuracy"]]
  
  if (class(cm[["byClass"]]) == "numeric") {
    svml_medianF1     <- median(cm[["byClass"]]["F1"], na.rm = TRUE)
  } else {
    svml_medianF1     <- median(cm[["byClass"]][,"F1"], na.rm = TRUE)
  }

  toc(log=TRUE, quiet = TRUE)
  logger.info("Info: Finished training svmLinear with dataset %s%s", dataset_name, tic.log()[[1]])
  tic.clearlog()
}

```


### Support Vector Machine - Radial


```{r, model_svmRadial}

model_svmRadial <- NULL
svmr_accuracy   <- 0
svmr_medianF1   <- 0

if (TRAIN_SVMR) {
  logger.info("Info: Started training svm Radial with dataset %s", dataset_name)
  tic(msg = "", quiet = TRUE)
  
  model_svmRadial <- train(ident ~ ., 
                           data      = pp_train_data, 
                           method    = "svmRadial",
                           trControl = trainControl("cv", 
                                                    number = 5, 
                                                    classProbs = TRUE,
                                                    verboseIter = TRUE))
   
  print(model_svmRadial)
  print(model_svmRadial$finalModel)
  plot(varImp(model_svmRadial), main="Variable Importance")
  
  predicted_classes <- predict(model_svmRadial, pp_test_data[,-1])
  
  cm <- confusionMatrix(pp_test_data[,1],
                        predicted_classes,
                        mode = "everything",
                        dnn=c("Reference", "Predicted"))
  cm
  svmr_accuracy <- cm[["overall"]][["Accuracy"]]
  
  if (class(cm[["byClass"]]) == "numeric") {
    svmr_medianF1     <- median(cm[["byClass"]]["F1"], na.rm = TRUE)
  } else {
    svmr_medianF1     <- median(cm[["byClass"]][,"F1"], na.rm = TRUE)
  }

  toc(log=TRUE, quiet = TRUE)
  logger.info("Info: Finished training svm Radial with dataset %s. %s", dataset_name, tic.log()[[1]])
  tic.clearlog()
}

```


### Random Forest

```{r, model_rf}

model_rf    <- NULL
rf_accuracy <- 0
rf_medianF1 <- 0

if (TRAIN_RF) {
  logger.info("Info: Started training Random Forest with dataset %s", dataset_name)
  tic(msg = "", quiet = TRUE)
  
  model_rf    <- train(ident ~ ., 
                       data      = pp_train_data, 
                       method    = "rf",
                       trControl = trainControl("cv", 
                                                number = 5, 
                                                classProbs = TRUE,
                                                verboseIter = TRUE))
  print(model_rf)
  print(model_rf$finalModel)
  plot(varImp(model_rf), main="Variable Importance")
  
  model_rf$preProcess$rotation
  
  predicted_classes <- predict(model_rf, pp_test_data[,-1])
  
  cm <- confusionMatrix(pp_test_data[,1],
                        predicted_classes,
                        mode = "everything",
                        dnn=c("Reference", "Predicted"))
  cm
  rf_accuracy <- cm[["overall"]][["Accuracy"]]
  
  if (class(cm[["byClass"]]) == "numeric") {
    rf_medianF1     <- median(cm[["byClass"]]["F1"], na.rm = TRUE)
  } else {
    rf_medianF1     <- median(cm[["byClass"]][,"F1"], na.rm = TRUE)
  }

  toc(log=TRUE, quiet = TRUE)
  logger.info("Info: Finished training Random Forest with dataset %s%s", dataset_name, tic.log()[[1]])
  tic.clearlog()
}

```


### Gradient Boost Machine

```{r, model_gbm}

model_gbm    <- NULL
gbm_accuracy <- 0
gbm_medianF1 <- 0

if (TRAIN_GBM) {
  logger.info("Info: Started training gbm with dataset %s", dataset_name)
  tic(msg = "", quiet = TRUE)
  
  model_gbm    <- train(ident ~ ., 
                        data      = pp_train_data, 
                        method    = "gbm",
                        trControl = trainControl("cv", 
                                                 number = 5, 
                                                 classProbs = TRUE,
                                                 verboseIter = TRUE))

  print(model_gbm)
  print(model_gbm$finalModel)
  
  # This plot does not work for gbm
  # plot(varImp(model_gbm), main="Variable Importance")
  
  predicted_classes <- predict(model_gbm, pp_test_data[,-1])
  cm <- confusionMatrix(pp_test_data[,1],
                        predicted_classes,
                        mode = "everything",
                        dnn=c("Reference", "Predicted"))
  cm
  gbm_accuracy <- cm[["overall"]][["Accuracy"]]
  
  if (class(cm[["byClass"]]) == "numeric") {
    gbmnet_medianF1     <- median(cm[["byClass"]]["F1"], na.rm = TRUE)
  } else {
    gbmnet_medianF1     <- median(cm[["byClass"]][,"F1"], na.rm = TRUE)
  }

  toc(log=TRUE, quiet = TRUE)
  logger.info("Info: Finished training gbm with dataset %s. %s", dataset_name, tic.log()[[1]])
  tic.clearlog()
}

```


### Generalized Linear Models net

```{r, model_glmnet}

model_glmnet    <- NULL
glmnet_accuracy <- 0
glmnet_medianF1 <- 0

if (TRAIN_GLMNET) {
  logger.info("Info: Started training glmnet with dataset %s", dataset_name)
  tic(msg = "", quiet = TRUE)
  
    model_glmnet <- train(ident ~ ., 
                        data      = pp_train_data, 
                        method    = "glmnet",
                        tuneGrid  = expand.grid(alpha = 0:1, lambda = 0.10 / 10),
                        trControl = trainControl("cv", 
                                                 number = 5, 
                                                 classProbs = TRUE,
                                                 verboseIter = FALSE))

  print(model_glmnet)
  print(model_glmnet$finalModel)
  
  # This plot does not work for gbm
  # plot(varImp(model_gbm), main="Variable Importance")
  
  predicted_classes <- predict(model_glmnet, pp_test_data[,-1])
  cm <- confusionMatrix(pp_test_data[,1],
                        predicted_classes,
                        mode = "everything",
                        dnn=c("Reference", "Predicted"))
  cm
  glmnet_accuracy <- cm[["overall"]][["Accuracy"]]
  
  if (class(cm[["byClass"]]) == "numeric") {
    glmnet_medianF1     <- median(cm[["byClass"]]["F1"], na.rm = TRUE)
  } else {
    glmnet_medianF1     <- median(cm[["byClass"]][,"F1"], na.rm = TRUE)
  }
  
  toc(log = TRUE, quiet = TRUE)
  logger.info("Info: Finished training glmnet with dataset %s. %s", dataset_name, tic.log()[[1]])
  tic.clearlog()
}

```


# Save the models, data and the configuration file

```{r, save_models }

if (SAVE) {
  save_csv <- config_data["minCount", "Value"]           != minCount ||
              config_data["maxCount", "Value"]           != maxCount ||
              config_data["minFeature", "Value"]         != minFeature ||
              config_data["maxFeature", "Value"]         != maxFeature ||
              config_data["min_class_size", "Value"]     != min_class_size ||
              config_data["cluster_resolution", "Value"] != cluster_resolution 

  if (save_csv) {
    config_data["min_class_size", "Value"]     <- min_class_size
    config_data["cluster_resolution", "Value"] <- cluster_resolution
    config_data["minCount", "Value"]           <- minCount
    config_data["maxCount", "Value"]           <- maxCount
    config_data["minFeature", "Value"]         <- minFeature
    config_data["maxFeature", "Value"]         <- maxFeature
    write.csv(config_data,
              file.path(config_path, paste0(dataset_name, "_config.csv")),
              row.names = TRUE)
  }
  
  model_information = list(
    seurat_object      = seurat_object,
    top_genes          = top_genes,
    config_data        = config_data,
    preproc_model      = preproc_model,
    model_rf           = model_rf,
    model_gbm          = model_gbm,
    model_svmRadial    = model_svmRadial,
    model_svmLinear    = model_svmLinear,
    model_glmnet       = model_glmnet,
    dataset_name       = dataset_name,
    PCA_Threshold      = PCA_Threshold,
    features_limit     = features_limit,
    cluster_resolution = cluster_resolution
    )
    
  # Now save the models for later use

  save(model_information, 
       file = file.path(rdata_path, paste0(dataset_name, "_models_", preproc_method, ".rData")))
}

```


```{r}

logger.info("")
logger.info("Summary: Model created with dataset %s", dataset_name)
logger.info("")

logger.info("   Config Parameters")  
logger.info("      min_class_size     : %d",    min_class_size)
logger.info("      cluster_resolution : %2.1f", cluster_resolution)
logger.info("      minCount           : %d",    minCount)
logger.info("      maxCount           : %d",    maxCount)
logger.info("      minFeature         : %d",    minFeature)
logger.info("      maxFeature         : %d",    maxFeature)
logger.info("")

logger.info("   Preprocess")  
logger.info("      Method             : %s", preproc_method)
logger.info("")

logger.info("   Train Parameters")  
logger.info("      PCA Threshold      : %2.1f", PCA_Threshold)
logger.info("      Features used      : %d",    features_limit)
logger.info("      PCA dimensions     : %d",    dim(pp_train_data)[2])
logger.info("")

logger.info("   Accuracy")
logger.info("      Accuracy of Random Forest is     : %3.1f%%", rf_accuracy * 100)
logger.info("      Accuracy of GBM is               : %3.1f%%", gbm_accuracy * 100)
logger.info("      Accuracy of SVM Radial is        : %3.1f%%", svmr_accuracy * 100)
logger.info("      Accuracy of SVM Linear is        : %3.1f%%", svml_accuracy * 100)
logger.info("      Accuracy of GLMNET is            : %3.1f%%", glmnet_accuracy * 100)
logger.info("")
logger.info("      Median F1 of Random Forest is    : %3.1f%%", rf_medianF1 * 100)
logger.info("      Median F1 of GBM is              : %3.1f%%", gbm_medianF1 * 100)
logger.info("      Median F1 of SVM Radial is       : %3.1f%%", svmr_medianF1 * 100)
logger.info("      Median F1 of SVM Linear is       : %3.1f%%", svml_medianF1 * 100)
logger.info("      Median F1 of GLMNET is           : %3.1f%%", glmnet_medianF1 * 100)

```




