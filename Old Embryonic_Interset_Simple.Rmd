---
title: "Classify and Evaluate - All"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include = FALSE} 
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# rm(list = ls())
set.seed(1234)
```

Make sure to set the knit directory to 'project directory' (which is the directory below 'Code')
Run command: rmarkdown::render("Caret_Predict.Rmd")


# Specify key use parameters


```{r}

# Specify all the pairs of datasets and models to evaluate

to_review <- tribble(
  ~ModelData,             ~TestData,
  
  "Bo",                  "Bo",
  "Zhou",                "Zhou",
  "Petropoulos1",        "Petropoulos1",

  "Bo",                  "Zhou",
  "Bo",                  "Petropoulos1",
  
  "Zhou",                "Bo",
  "Zhou",                "Petropoulos1",
  
  "Petropoulos1",        "Bo",
  "Petropoulos1",        "Zhou"
)

# Select preprocessing
preproc_method   <- "pca"

# Select model
method           <- "gbm"

```


```{r}

to_review <- rowid_to_column(to_review, "nr")
to_review <- as.data.frame(to_review)
to_review

```

# Initialise

```{r, libraries, message=FALSE}

library(readr)
library(tidyverse)
library(gridExtra)
library(caret)
library(matrixStats)
library(GGally)

source("Code/Predict/read_model_and_data.R")
source("Code/utilities.R")
```


```{r, set preprocess options}

if (preproc_method == "pca")  {
  log_preprocess   <- FALSE
  caret_preprocess_method <- c('scale', 'center', 'pca')
} else if (preproc_method == "nopca")  {
  log_preprocess   <- FALSE
  caret_preprocess_method <- c('scale', 'center')
} else if (preproc_method == "logpca")  {
  log_preprocess   <- TRUE
  caret_preprocess_method <- c('scale', 'center', 'pca')
}

```


```{r, path to set and report_out}

proj_path       <- file.path(".")
data_path       <- file.path(proj_path, "DataSets")
log_path        <- file.path(proj_path, "Logs")
rdata_path      <- file.path(proj_path, "rData")
config_path     <- file.path(proj_path, "config")

# Initialise the report_out table
report_out      <- data.frame()

# Information of all the runs will be gathered in the class_summaries list  
class_summaries <- list()

# Information of all missing genes is stored
missing_cols    <- data.frame()

```

# Read and Predict 


```{r, fig.width = 5}

#sink(file.path(log_path, paste0(preproc_method, "_all_classify.txt")))

for (row in 1:nrow(to_review)) {
  
  # Set the data and model name and read the data
  model_name      <- to_review[row, "ModelData"]
  dataset_name    <- to_review[row, "TestData"]
  
  #ret            <- read_model_and_data(rdata_path, model_name, dataset_name, preproc_method)
  retd             <-load_single_data(rdata_path, dataset_name )
  retm            <- load_single_model(rdata_path, model_name, method, preproc_method)
  
  preproc_model   <- retm$model_information[["preproc_model"]]
  model           <- retm$model_information[["model"]]
  features_limit  <- retm$model_information[["features_limit"]]
  PCA_Threshold   <- retm$model_information[["PCA_Threshold"]]
  top_genes       <- retm$model_information[["top_genes"]]
  
  data            <- retd$data
  labels          <- retd$labels
  
  if (log_preprocess) {
    data <- log(1 + data)
  }
  
  # In case the model and data set are the same, you do not want to predict on the whole dataset, but only on the part
  # that was not used for training.
  # Now you use the whole dataset (and also the part that you used to train, so there will be an oveestimation of median F1 in these cases
  
  # if (model_name == dataset_name) {
  #   data   <- data[-trainRowNumbers,]
  #   labels <- labels[-trainRowNumbers,]
  #   labels <- as.data.frame(labels)
  # }
  
  
  # Only needed when dataset_names is not model_name, i.e. in case of inter-set predicting 
  # Check if there are top genes missing in the dataset, and if so provide dummy values 
  
  nr_missing_col <- 0
  if (model_name != dataset_name) {
    missing_col <- setdiff(top_genes[1:features_limit], colnames(data))
    newcol = rep.int(0,dim(data)[1])
    if (length(missing_col) > 0) {
      for (i in 1:length(missing_col)) {
        data[ , missing_col[i]] = newcol
      }
      cat(sprintf("There were %s top %d features missing in the data datset.\n", length(missing_col), features_limit))
    } else {
      cat(sprintf("All top %d features were present in the data set.\n", features_limit))
    }
    nr_missing_col <- length(missing_col)
  }
  missing_col  <- data.frame(ModelData = model_name, TestData = dataset_name, Missing = nr_missing_col)
  missing_cols <- rbind(missing_cols, missing_col)
  
  ######################################################################################################################
  # Do the actual predicting
  ######################################################################################################################
  
  # First preprocess
  pp_data           <- predict(preproc_model, data)
  
  # Then predict
  predicted_classes <- predict(model, pp_data)
  probability       <- predict(model, pp_data, type = "prob")
  
  ######################################################################################################################
  # Overrule the classification if the probability is too low
  ######################################################################################################################
  
  min_prob_value               <- 0.6
  min_prob_ratio               <- 4
  prob_max                     <- rowMaxs(as.matrix(probability))
  prob_ratio                   <- apply(probability,
                                        1,
                                        function (x) {
                                          sp = sort(x, decreasing = TRUE)
                                          return (sp[1]/sp[2])
                                        })
  reliable                     <- prob_max > min_prob_value  | prob_ratio > min_prob_ratio
  corrected_classes            <- predicted_classes
  levels(corrected_classes)    <- c(levels(predicted_classes), "Unassigned")
  corrected_classes[!reliable] <- "Unassigned"
  correct                      <- as.character(predicted_classes) == as.character(labels[,1])
  
  ######################################################################################################################
  # Get the prediction result nicely together in the 'Predicted' table
  ######################################################################################################################
  
  Predicted  <- cbind("ReferenceClass" = labels[,1], 
                      "PredictedClass" = predicted_classes, 
                      "Reliable"       = reliable,
                      "CorrectedClass" = corrected_classes, 
                      "Correct"        = correct,
                      "Max"            = prob_max, 
                      "Ratio"          = prob_ratio,
                      probability)
  
  # Calculate the percentages
  assigned_cells               <- Predicted[Predicted$CorrectedClass != "Unassigned",]
  predicted_count              <- dim(probability)[1]
  assigned_count               <- sum(reliable)
  unassigned_count             <- predicted_count - assigned_count 
  correct_predicted            <- sum(as.character(Predicted$PredictedClass) == as.character(Predicted$ReferenceClass))
  correct_assigned             <- sum(as.character(assigned_cells$PredictedClass) == as.character(assigned_cells$ReferenceClass))
  
  correct_predicted_percentage <- (correct_predicted / predicted_count) * 100
  correct_assigned_percentage  <- (correct_assigned / assigned_count) * 100
  unassigned_percentage        <- unassigned_count / predicted_count * 100
  
  ######################################################################################################################
  # Store information in the class summary table
  ######################################################################################################################
  
  class_summary <- data.frame()
  
  for (class in colnames(probability)) {
    new_rec = data.frame(
      average_prob    = mean(Predicted[which((Predicted$PredictedClass == class)), "Max"]),
      prob_correct    = mean(Predicted[which((Predicted$PredictedClass == class) & (Predicted$Correct  == TRUE)),  "Max"]),
      prob_incorrect  = mean(Predicted[which((Predicted$PredictedClass == class) & (Predicted$Correct  == FALSE)), "Max"]),
      prob_reliable   = mean(Predicted[which((Predicted$PredictedClass == class) & (Predicted$Reliable == TRUE)),  "Max"]),
      prob_unreliable = mean(Predicted[which((Predicted$PredictedClass == class) & (Predicted$Reliable == FALSE)), "Max"]),
      
      nr_correct      = nrow(Predicted[which((Predicted$PredictedClass == class) & (Predicted$Correct  == TRUE)),  ]),
      nr_incorrect    = nrow(Predicted[which((Predicted$PredictedClass == class) & (Predicted$Correct  == FALSE)), ]),
      nr_reliabe      = nrow(Predicted[which((Predicted$PredictedClass == class) & (Predicted$Reliable == TRUE)),  ]),
      nr_unreliabe    = nrow(Predicted[which((Predicted$PredictedClass == class) & (Predicted$Reliable == FALSE)), ]))
    rownames(new_rec) <- class
    if ((new_rec$nr_correct + new_rec$nr_incorrect) != 0) {
      class_summary <- rbind(class_summary, new_rec)
    }
  }
  colnames(class_summary) = c('Average Prob', 
                              'Prob (Correct)',
                              'Prob (incorrect)',
                              'Prob (Reliable)',
                              'Prob (Unreliable)',
                              'Nr (Correct)',
                              'Nr (Incorrect)',
                              'Nr (Reliabe)',
                              'Nr (Unreliabe)')
  class_summaries[[paste0(method, "-", dataset_name,  "-", model_name)]] <- class_summary
  
  ######################################################################################################################
  # Plot the results in the Predict / Reference graphs
  ######################################################################################################################
  
  CorrectlyPredicted   <- Predicted[Predicted$Correct == TRUE,]
  IncorrectlyPredicted <- Predicted[Predicted$Correct == FALSE,]
  
  p1 <- ggplot() + 
    geom_jitter(data = CorrectlyPredicted, 
                mapping = aes(x = PredictedClass, y = Max),
                width   = 0.2,
                height  = 0,
                alpha   = 0.7,
                size    = 0.6,
                color   = 'green') +
    geom_jitter(data = IncorrectlyPredicted, 
                mapping = aes(x = PredictedClass, y = Max),
                width   = 0.2,
                height  = 0,
                alpha   = 0.7,
                size    = 0.6,
                color   = 'red') +
    labs(x = "Predicted") +
    labs(y = "Probability of prediction") +
    scale_y_continuous(limits = c(0, 1.0), breaks = seq(0, 1.0, 0.1)) +
    theme_light() +
    theme(axis.text.x = element_text(angle = 45, hjust  = 1)) 
  
  p2 <- ggplot() + 
    geom_jitter(data = CorrectlyPredicted, 
                mapping = aes(x = ReferenceClass, y = Max),
                width = 0.3,
                height = 0,
                alpha = 0.7,
                size  = 0.6,
                color = 'green') +
    geom_jitter(data = IncorrectlyPredicted, 
                mapping = aes(x = ReferenceClass, y = Max),
                width = 0.3,
                height = 0,
                alpha = 0.7, 
                size  = 0.6,
                color = 'red') +
    labs(x = "Reference") +
    labs(y = "Probability of prediction") +
    scale_y_continuous(limits = c(0, 1.0), breaks = seq(0, 1.0, 0.1)) +
    theme_light() +
    theme(axis.text.x = element_text(angle = 45, hjust  = 1)) 
  
  title_string <- sprintf("Dataset '%s' predicted with model data '%s', using method '%s' (%d features, PCA threshold of %2.1f)", 
                          dataset_name, model_name, method, features_limit, PCA_Threshold)
  grid.arrange(p1, p2, ncol=2, top = title_string)
  
  ######################################################################################################################
  # Get the confusion matrix
  ######################################################################################################################
  
  # Before callingconfusionMatrix add missing classes to make sure bot Predicted and Regference have the same classes 
  
  levelsP  <- levels(Predicted$PredictedClass)
  levelsR  <- levels(Predicted$ReferenceClass)
  add_to_R <- setdiff(levelsP, levelsR)
  add_to_P <- setdiff(levelsR, levelsP)
  
  levels(Predicted$PredictedClass) <- c(levels(Predicted$PredictedClass), add_to_P)
  levels(Predicted$ReferenceClass) <- c(levels(Predicted$ReferenceClass), add_to_R)
  
  cm <- confusionMatrix(Predicted$PredictedClass,
                        Predicted$ReferenceClass,
                        mode = "everything",
                        dnn = c("Predicted", "Reference"))
  
  cat(sprintf("\n\n\n\n"))
  cat(sprintf("*************************************************************************************************************\n"))
  cat(sprintf("Confusion Matrix for %s, model %s, and dataset %s\n", method, model_name, dataset_name))
  cat(sprintf("*************************************************************************************************************\n\n"))
  print(cm)
  
  ######################################################################################################################
  # Fill the report-out table
  ######################################################################################################################
  
  Pred1        <- Predicted[ ,c("Reliable", "Correct")]
  Accuracy     <- dim(Pred1[which(Pred1$Correct == TRUE),])[1] / dim(Pred1)[1]
   
  Pred2        <- Predicted[Predicted$Reliable == TRUE, c("Reliable", "Correct")]
  CorrAccuracy <- dim(Pred2[which(Pred2$Correct == TRUE),])[1]  / dim(Pred2)[1]
  new          <- data.frame(Method       = method,
                             TestData     = dataset_name,
                             ModelData    = model_name,
                             Accuracy     = Accuracy,
                             CorrAccuracy = CorrAccuracy,
                             Confidence   = mean(Predicted$Max),
                             medianF1     <- median(cm[["byClass"]][,"F1"], na.rm = TRUE),
                             meanF1       <- mean(cm[["byClass"]][,"F1"], na.rm = TRUE)) 
   
  report_out   <- rbind(report_out, new)
}

#sink()

```
# Report out

```{r, fig.width = 6}

print(class_summaries)

```





```{r}
report_out <- report_out %>% 
  select(ModelData, TestData, medianF1) %>% 
  arrange(ModelData)

report <- inner_join(report_out, missing_cols) %>%
  mutate(Genes = features_limit - Missing) %>%
  select(-Missing) %>%
  rename("Model" = ModelData )

report

```

```{r}
ggplot(report, aes(x = TestData, y = medianF1)) +
  geom_bar(stat="identity", width = 0.6 ) +
  scale_y_continuous(limits = c(0, 1)) + 
  facet_grid(Model ~ .) +
  theme_light()
```

```{r}
ggplot(report, aes(x = TestData, y = medianF1, fill = Model, dotsize = Genes)) +
  geom_dotplot(binaxis='y', 
               stackdir='center', 
               binwidth = .05,
               fill="red") +
  scale_y_continuous(limits = c(0, 1)) + 
  facet_grid(Model ~ .) +
  theme_light() +
  labs(y = "Median F1") + 
  labs(x = "Test Dataset")
```


```{r}
  report$Genes <- as.factor(report$Genes)
  ggplot(report, aes(x = TestData, y = medianF1, col = Model, size = Genes)) +
    geom_point() +
    scale_y_continuous(limits = c(0, 1.05)) + 
    facet_grid(Model ~ .) +
    theme_light() +
    labs(y = "Median F1") +  
    labs(x = "Test Dataset")
```




